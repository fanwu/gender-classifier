{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "95adf691-f162-4d4d-a843-f47f3c4c3d21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T04:56:54.745683Z",
     "iopub.status.busy": "2025-09-07T04:56:54.745431Z",
     "iopub.status.idle": "2025-09-07T04:56:56.380260Z",
     "shell.execute_reply": "2025-09-07T04:56:56.379501Z",
     "shell.execute_reply.started": "2025-09-07T04:56:54.745664Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.12/site-packages (4.55.2)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.12/site-packages (2.2.1)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.12/site-packages (0.21.0)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.12/site-packages (11.3.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.12/site-packages (1.7.1)\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.12/site-packages (1.37.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (2025.7.34)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.12/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.12/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (from datasets) (2.3.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.12/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.12/site-packages (from datasets) (3.12.15)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.12/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: urllib3>=1.25.10 in /opt/conda/lib/python3.12/site-packages (from responses<0.19->datasets) (1.26.19)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy!=1.13.2,>=1.13.1 in /opt/conda/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn) (1.16.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: botocore<1.38.0,>=1.37.1 in /opt/conda/lib/python3.12/site-packages (from boto3) (1.37.1)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.12/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /opt/conda/lib/python3.12/site-packages (from boto3) (0.11.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.12/site-packages (from botocore<1.38.0,>=1.37.1->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.38.0,>=1.37.1->boto3) (1.17.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets) (1.20.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy!=1.13.2,>=1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install transformers datasets torch torchvision pillow scikit-learn boto3\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import pipeline, ViTImageProcessor, ViTForImageClassification, Trainer, TrainingArguments\n",
    "from PIL import Image\n",
    "import boto3\n",
    "import io\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f128d9bd-b4da-4fd7-8945-7c3e7d4451dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T04:57:09.697947Z",
     "iopub.status.busy": "2025-09-07T04:57:09.697635Z",
     "iopub.status.idle": "2025-09-07T04:57:09.709482Z",
     "shell.execute_reply": "2025-09-07T04:57:09.708874Z",
     "shell.execute_reply.started": "2025-09-07T04:57:09.697921Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# S3 Configuration\n",
    "BUCKET_NAME = \"fanwu-ml-test\"  # Replace with your bucket name\n",
    "S3_PREFIX = \"gender-data/\"  # Optional: prefix for organizing data in S3\n",
    "\n",
    "# Initialize S3 client\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0a5dda62-336c-454f-a5a6-57fd37b20d3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T04:57:18.100180Z",
     "iopub.status.busy": "2025-09-07T04:57:18.099912Z",
     "iopub.status.idle": "2025-09-07T04:57:18.105384Z",
     "shell.execute_reply": "2025-09-07T04:57:18.104843Z",
     "shell.execute_reply.started": "2025-09-07T04:57:18.100158Z"
    }
   },
   "outputs": [],
   "source": [
    "class S3GenderDataset(Dataset):\n",
    "    def __init__(self, s3_paths, labels, processor, bucket_name):\n",
    "        self.s3_paths = s3_paths\n",
    "        self.labels = labels\n",
    "        self.processor = processor\n",
    "        self.bucket_name = bucket_name\n",
    "        self.s3 = boto3.client('s3')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.s3_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Download image from S3\n",
    "        try:\n",
    "            response = self.s3.get_object(Bucket=self.bucket_name, Key=self.s3_paths[idx])\n",
    "            image_data = response['Body'].read()\n",
    "            image = Image.open(io.BytesIO(image_data)).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {self.s3_paths[idx]}: {e}\")\n",
    "            # Return a blank image if there's an error\n",
    "            image = Image.new('RGB', (224, 224), color='white')\n",
    "        \n",
    "        # Process image\n",
    "        inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "        \n",
    "        return {\n",
    "            'pixel_values': inputs['pixel_values'].squeeze(0),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8507f07e-6722-4126-a5a8-003e621e70c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T04:57:29.587490Z",
     "iopub.status.busy": "2025-09-07T04:57:29.587226Z",
     "iopub.status.idle": "2025-09-07T04:57:29.593275Z",
     "shell.execute_reply": "2025-09-07T04:57:29.592704Z",
     "shell.execute_reply.started": "2025-09-07T04:57:29.587470Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Add to your dataset class\n",
    "class S3GenderDatasetWithAugmentation(Dataset):\n",
    "    def __init__(self, s3_paths, labels, processor, bucket_name, is_training=True):\n",
    "        self.s3_paths = s3_paths\n",
    "        self.labels = labels\n",
    "        self.processor = processor\n",
    "        self.bucket_name = bucket_name\n",
    "        self.s3 = boto3.client('s3')\n",
    "        self.is_training = is_training\n",
    "        \n",
    "        # Data augmentation for training\n",
    "        if is_training:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomRotation(degrees=10),\n",
    "                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "                transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "            ])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.s3_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Download image from S3\n",
    "        response = self.s3.get_object(Bucket=self.bucket_name, Key=self.s3_paths[idx])\n",
    "        image_data = response['Body'].read()\n",
    "        image = Image.open(io.BytesIO(image_data)).convert('RGB')\n",
    "        \n",
    "        # Apply augmentation\n",
    "        image = self.transform(image)\n",
    "        \n",
    "        # Process image\n",
    "        inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "        \n",
    "        return {\n",
    "            'pixel_values': inputs['pixel_values'].squeeze(0),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "058d8ff7-a392-4509-a63d-5322f1518ac8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T04:57:37.192382Z",
     "iopub.status.busy": "2025-09-07T04:57:37.192126Z",
     "iopub.status.idle": "2025-09-07T04:57:37.197726Z",
     "shell.execute_reply": "2025-09-07T04:57:37.197169Z",
     "shell.execute_reply.started": "2025-09-07T04:57:37.192362Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data_from_s3(bucket_name, prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Load images from S3 bucket with structure:\n",
    "    s3://bucket/prefix/male/image1.jpg\n",
    "    s3://bucket/prefix/female/image2.jpg\n",
    "    \n",
    "    Or flat structure with labels in filenames:\n",
    "    s3://bucket/prefix/male_image1.jpg\n",
    "    s3://bucket/prefix/female_image2.jpg\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    \n",
    "    # List all objects in the bucket with the given prefix\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    pages = paginator.paginate(Bucket=bucket_name, Prefix=prefix)\n",
    "    \n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    \n",
    "    for page in pages:\n",
    "        if 'Contents' in page:\n",
    "            for obj in page['Contents']:\n",
    "                key = obj['Key']\n",
    "                \n",
    "                # Skip directories and non-image files\n",
    "                if key.endswith('/') or not key.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n",
    "                    continue\n",
    "                \n",
    "                # Extract label from path\n",
    "                # Method 1: From folder structure (e.g., prefix/male/image.jpg)\n",
    "                if '/male/' in key or key.endswith('/male'):\n",
    "                    labels.append(0)  # male = 0\n",
    "                    image_paths.append(key)\n",
    "                elif '/female/' in key or key.endswith('/female'):\n",
    "                    labels.append(1)  # female = 1\n",
    "                    image_paths.append(key)\n",
    "                # Method 2: From filename (e.g., male_image1.jpg)\n",
    "                elif 'male_' in key.lower():\n",
    "                    labels.append(0)\n",
    "                    image_paths.append(key)\n",
    "                elif 'female_' in key.lower():\n",
    "                    labels.append(1)\n",
    "                    image_paths.append(key)\n",
    "                else:\n",
    "                    print(f\"Skipping image with unclear label: {key}\")\n",
    "    \n",
    "    label_map = {0: \"male\", 1: \"female\"}\n",
    "    return image_paths, labels, label_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6943db9c-32d8-46e4-81ce-0e4dc42684fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T04:57:43.592430Z",
     "iopub.status.busy": "2025-09-07T04:57:43.592169Z",
     "iopub.status.idle": "2025-09-07T04:57:43.757895Z",
     "shell.execute_reply": "2025-09-07T04:57:43.757256Z",
     "shell.execute_reply.started": "2025-09-07T04:57:43.592409Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = \"google/vit-base-patch16-224\"\n",
    "\n",
    "# Load pre-trained processor and model\n",
    "processor = ViTImageProcessor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Load the base model first (with original 1000 classes)\n",
    "model = ViTForImageClassification.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Replace the classifier head for 2 classes\n",
    "model.classifier = torch.nn.Linear(model.config.hidden_size, 2)\n",
    "\n",
    "# Update the config for our new task\n",
    "model.config.num_labels = 2\n",
    "model.config.id2label = {0: \"male\", 1: \"female\"}\n",
    "model.config.label2id = {\"male\": 0, \"female\": 1}\n",
    "\n",
    "# IMPORTANT: Update the num_labels attribute in the model itself\n",
    "model.num_labels = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7b93032c-be76-448a-b918-a3e76f8613a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T04:57:55.848972Z",
     "iopub.status.busy": "2025-09-07T04:57:55.848678Z",
     "iopub.status.idle": "2025-09-07T05:00:05.890567Z",
     "shell.execute_reply": "2025-09-07T05:00:05.889980Z",
     "shell.execute_reply.started": "2025-09-07T04:57:55.848950Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from S3...\n",
      "Total images found in S3: 198\n",
      "Label distribution: {0: 81, 1: 117}\n",
      "Training images: 158\n",
      "Validation images: 40\n",
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='395' max='395' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [395/395 01:50, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.739000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.581800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.448400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.215900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.149000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.292200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.064700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.014600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.009600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.017900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.052500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.008200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.004400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.009300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded config.json to S3\n",
      "Uploaded model.safetensors to S3\n",
      "Uploaded preprocessor_config.json to S3\n",
      "Training completed and model saved to S3!\n"
     ]
    }
   ],
   "source": [
    "# Load data directly from S3\n",
    "print(\"Loading data from S3...\")\n",
    "image_paths, labels, label_map = load_data_from_s3(BUCKET_NAME, S3_PREFIX)\n",
    "print(f\"Total images found in S3: {len(image_paths)}\")\n",
    "print(f\"Label distribution: {dict(zip(*np.unique(labels, return_counts=True)))}\")\n",
    "\n",
    "if len(image_paths) == 0:\n",
    "    print(\"No images found! Check your bucket name and S3 structure.\")\n",
    "    print(\"Expected structure:\")\n",
    "    print(\"s3://your-bucket/prefix/male/image1.jpg\")\n",
    "    print(\"s3://your-bucket/prefix/female/image2.jpg\")\n",
    "else:\n",
    "    # Split data\n",
    "    train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "        image_paths, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "    )\n",
    "    \n",
    "    print(f\"Training images: {len(train_paths)}\")\n",
    "    print(f\"Validation images: {len(val_paths)}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = S3GenderDataset(train_paths, train_labels, processor, BUCKET_NAME)\n",
    "    val_dataset = S3GenderDataset(val_paths, val_labels, processor, BUCKET_NAME)\n",
    "    \n",
    "    # Compute metrics function\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        return {\"accuracy\": accuracy_score(labels, predictions)}\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./gender-classification-vit-s3\",\n",
    "        num_train_epochs=5,  # Reduced for small dataset\n",
    "        per_device_train_batch_size=2,  # Smaller batch size\n",
    "        per_device_eval_batch_size=2,\n",
    "        warmup_steps=10,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=10,\n",
    "        eval_strategy=\"no\",  # Disable evaluation to save space\n",
    "        save_strategy=\"no\",  # Disable intermediate saving\n",
    "        save_total_limit=1,  # Only keep 1 checkpoint\n",
    "        remove_unused_columns=False,\n",
    "        dataloader_pin_memory=False,\n",
    "        learning_rate=2e-5,\n",
    "        dataloader_num_workers=0,\n",
    "        report_to=None,  # Disable wandb/tensorboard logging\n",
    "    )\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save model to S3\n",
    "    model.save_pretrained(\"./gender-classification-final\")\n",
    "    processor.save_pretrained(\"./gender-classification-final\")\n",
    "    \n",
    "    # Upload trained model to S3\n",
    "    import os\n",
    "    for root, dirs, files in os.walk(\"./gender-classification-final\"):\n",
    "        for file in files:\n",
    "            local_path = os.path.join(root, file)\n",
    "            s3_key = f\"models/gender-classification/{file}\"\n",
    "            s3.upload_file(local_path, BUCKET_NAME, s3_key)\n",
    "            print(f\"Uploaded {file} to S3\")\n",
    "    \n",
    "    print(\"Training completed and model saved to S3!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "424fda7c-67f3-4c8f-8cdf-c0a9e1d363f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T05:00:15.439491Z",
     "iopub.status.busy": "2025-09-07T05:00:15.439227Z",
     "iopub.status.idle": "2025-09-07T05:00:15.444771Z",
     "shell.execute_reply": "2025-09-07T05:00:15.444095Z",
     "shell.execute_reply.started": "2025-09-07T05:00:15.439469Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prediction function for S3 images\n",
    "def predict_gender_from_s3(s3_key, bucket_name, model, processor):\n",
    "    \"\"\"Predict gender for an image stored in S3\"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    \n",
    "    # Download image from S3\n",
    "    response = s3.get_object(Bucket=bucket_name, Key=s3_key)\n",
    "    image_data = response['Body'].read()\n",
    "    image = Image.open(io.BytesIO(image_data)).convert('RGB')\n",
    "    \n",
    "    # Process and predict\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "    # Move inputs to the same device as model\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predicted_class_id = predictions.argmax().item()\n",
    "        confidence = predictions.max().item()\n",
    "    \n",
    "    predicted_label = model.config.id2label[predicted_class_id]\n",
    "    return predicted_label, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7d945745-4afd-451c-8c4f-952ffdbe4848",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T05:00:21.456128Z",
     "iopub.status.busy": "2025-09-07T05:00:21.455858Z",
     "iopub.status.idle": "2025-09-07T05:00:21.463122Z",
     "shell.execute_reply": "2025-09-07T05:00:21.462344Z",
     "shell.execute_reply.started": "2025-09-07T05:00:21.456107Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_people_filtered(s3_key, bucket_name, detector):\n",
    "    \"\"\"Filter out distant people and false positives\"\"\"\n",
    "    try:\n",
    "        s3 = boto3.client('s3')\n",
    "        response = s3.get_object(Bucket=bucket_name, Key=s3_key)\n",
    "        image_data = response['Body'].read()\n",
    "        image = Image.open(io.BytesIO(image_data)).convert('RGB')\n",
    "        \n",
    "        # Get image dimensions\n",
    "        img_width, img_height = image.size\n",
    "        \n",
    "        detections = detector(image)\n",
    "        \n",
    "        valid_people = 0\n",
    "        \n",
    "        for detection in detections:\n",
    "            if 'person' in detection['label'].lower():\n",
    "                score = detection['score']\n",
    "                box = detection['box']\n",
    "                \n",
    "                # Calculate detection size\n",
    "                det_width = box['xmax'] - box['xmin']\n",
    "                det_height = box['ymax'] - box['ymin']\n",
    "                det_area = det_width * det_height\n",
    "                \n",
    "                # Calculate relative size (percentage of image)\n",
    "                relative_width = det_width / img_width\n",
    "                relative_height = det_height / img_height\n",
    "                relative_area = det_area / (img_width * img_height)\n",
    "                \n",
    "                # Filter criteria for close-up person photos\n",
    "                criteria_met = (\n",
    "                    score > 0.8 and  # High confidence only\n",
    "                    relative_area > 0.1 and  # At least 10% of image\n",
    "                    relative_height > 0.3 and  # Person takes up significant height\n",
    "                    box['ymin'] < img_height * 0.7  # Person not at very bottom (likely full body in frame)\n",
    "                )\n",
    "                \n",
    "                print(f\"  Detection: confidence={score:.2f}, area={relative_area:.2f}, height={relative_height:.2f}\")\n",
    "                \n",
    "                if criteria_met:\n",
    "                    valid_people += 1\n",
    "                    print(f\"    ✓ Valid person detected\")\n",
    "                else:\n",
    "                    print(f\"    ✗ Filtered out (likely distant/small)\")\n",
    "        \n",
    "        return valid_people\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "47efb0e5-5417-4e44-a870-877fe6c57f89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T05:00:24.692337Z",
     "iopub.status.busy": "2025-09-07T05:00:24.692074Z",
     "iopub.status.idle": "2025-09-07T05:00:24.698150Z",
     "shell.execute_reply": "2025-09-07T05:00:24.697427Z",
     "shell.execute_reply.started": "2025-09-07T05:00:24.692315Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_gender_from_s3_safe(s3_key, bucket_name, model, processor, detector):\n",
    "    \"\"\"Predict gender but reject multi-person images\"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    \n",
    "    try:\n",
    "        # Download image from S3\n",
    "        response = s3.get_object(Bucket=bucket_name, Key=s3_key)\n",
    "        image_data = response['Body'].read()\n",
    "        image = Image.open(io.BytesIO(image_data)).convert('RGB')\n",
    "       \n",
    "        face_count = count_people_filtered(s3_key, bucket_name, detector)\n",
    "        if face_count == 0:\n",
    "            return \"ERROR: No person detected in image\", 0.0\n",
    "        elif face_count > 1:\n",
    "            return f\"ERROR: Multiple people detected ({face_count} people). Please use single-person images.\", 0.0\n",
    "        \n",
    "        # If exactly 1 person, proceed with gender prediction\n",
    "        inputs = processor(images=image, return_tensors=\"pt\")\n",
    "        device = next(model.parameters()).device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            predicted_class_id = predictions.argmax().item()\n",
    "            confidence = predictions.max().item()\n",
    "        \n",
    "        predicted_label = model.config.id2label[predicted_class_id]\n",
    "        return predicted_label, confidence\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"ERROR: Failed to process image - {str(e)}\", 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d36323ac-fd0c-4541-a620-6ca7101e03b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T05:00:25.991223Z",
     "iopub.status.busy": "2025-09-07T05:00:25.990894Z",
     "iopub.status.idle": "2025-09-07T05:00:27.855914Z",
     "shell.execute_reply": "2025-09-07T05:00:27.855174Z",
     "shell.execute_reply.started": "2025-09-07T05:00:25.991195Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TESTING PREDICTIONS\n",
      "==================================================\n",
      "Image: 000001.png\n",
      "Predicted: female\n",
      "Confidence: 1.000\n",
      "------------------------------\n",
      "Image: 000002.png\n",
      "Predicted: female\n",
      "Confidence: 0.999\n",
      "------------------------------\n",
      "Image: 000004.png\n",
      "Predicted: female\n",
      "Confidence: 0.999\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test prediction on a sample image from your dataset\n",
    "if len(image_paths) > 0:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TESTING PREDICTIONS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Test on first few images\n",
    "    test_images = image_paths[:3]  # Test first 3 images\n",
    "    \n",
    "    for test_image in test_images:\n",
    "        try:\n",
    "            prediction, confidence = predict_gender_from_s3(test_image, BUCKET_NAME, model, processor)\n",
    "            print(f\"Image: {test_image.split('/')[-1]}\")\n",
    "            print(f\"Predicted: {prediction}\")\n",
    "            print(f\"Confidence: {confidence:.3f}\")\n",
    "            print(\"-\" * 30)\n",
    "        except Exception as e:\n",
    "            print(f\"Error predicting {test_image}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc526320-9247-4b3c-806c-689756152332",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T05:00:32.465643Z",
     "iopub.status.busy": "2025-09-07T05:00:32.465266Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting on all images in: s3://fanwu-ml-test/test/\n",
      "============================================================\n",
      "  Detection: confidence=1.00, area=0.26, height=0.92\n",
      "    ✓ Valid person detected\n",
      "  Detection: confidence=0.99, area=0.33, height=1.00\n",
      "    ✓ Valid person detected\n",
      "  Detection: confidence=0.78, area=0.57, height=1.00\n",
      "    ✗ Filtered out (likely distant/small)\n",
      "  Detection: confidence=0.99, area=0.25, height=0.91\n",
      "    ✓ Valid person detected\n",
      "  Detection: confidence=0.99, area=0.26, height=0.87\n",
      "    ✓ Valid person detected\n",
      "Image: 4-people.jpg\n",
      "Prediction: ERROR: Multiple people detected (4 people). Please use single-person images.\n",
      "Confidence: 0.000\n",
      "----------------------------------------\n",
      "Image: Longfellow-Bridge-Charles-River-Boston.jpeg\n",
      "Prediction: ERROR: No person detected in image\n",
      "Confidence: 0.000\n",
      "----------------------------------------\n",
      "Image: cat.jpg\n",
      "Prediction: ERROR: No person detected in image\n",
      "Confidence: 0.000\n",
      "----------------------------------------\n",
      "  Detection: confidence=0.52, area=0.43, height=0.54\n",
      "    ✗ Filtered out (likely distant/small)\n",
      "  Detection: confidence=0.99, area=0.49, height=0.62\n",
      "    ✓ Valid person detected\n",
      "Image: dudu.jpg\n",
      "Prediction: male\n",
      "Confidence: 0.928\n",
      "----------------------------------------\n",
      "  Detection: confidence=0.67, area=0.00, height=0.06\n",
      "    ✗ Filtered out (likely distant/small)\n",
      "  Detection: confidence=1.00, area=0.00, height=0.09\n",
      "    ✗ Filtered out (likely distant/small)\n",
      "  Detection: confidence=1.00, area=0.01, height=0.10\n",
      "    ✗ Filtered out (likely distant/small)\n",
      "  Detection: confidence=0.92, area=0.00, height=0.06\n",
      "    ✗ Filtered out (likely distant/small)\n",
      "  Detection: confidence=0.90, area=0.00, height=0.06\n",
      "    ✗ Filtered out (likely distant/small)\n",
      "  Detection: confidence=1.00, area=0.41, height=0.61\n",
      "    ✓ Valid person detected\n",
      "  Detection: confidence=0.69, area=0.00, height=0.06\n",
      "    ✗ Filtered out (likely distant/small)\n",
      "  Detection: confidence=1.00, area=0.02, height=0.20\n",
      "    ✗ Filtered out (likely distant/small)\n",
      "Image: kuma.jpg\n",
      "Prediction: female\n",
      "Confidence: 0.993\n",
      "----------------------------------------\n",
      "  Detection: confidence=0.60, area=0.83, height=0.86\n",
      "    ✗ Filtered out (likely distant/small)\n",
      "  Detection: confidence=0.99, area=0.66, height=0.68\n",
      "    ✓ Valid person detected\n",
      "Image: wufan1.jpg\n",
      "Prediction: male\n",
      "Confidence: 0.961\n",
      "----------------------------------------\n",
      "  Detection: confidence=1.00, area=0.96, height=0.96\n",
      "    ✓ Valid person detected\n",
      "Image: wufan2.jpeg\n",
      "Prediction: male\n",
      "Confidence: 0.989\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def predict_all_images_in_folder(s3_folder, bucket_name, model, processor, detector):\n",
    "    \"\"\"Predict gender for all images in an S3 folder\"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    \n",
    "    # List all images in the folder\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    pages = paginator.paginate(Bucket=bucket_name, Prefix=s3_folder)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(f\"Predicting on all images in: s3://{bucket_name}/{s3_folder}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for page in pages:\n",
    "        if 'Contents' in page:\n",
    "            for obj in page['Contents']:\n",
    "                key = obj['Key']\n",
    "                \n",
    "                # Skip directories and non-image files\n",
    "                if key.endswith('/') or not key.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # Get just the filename for display\n",
    "                    image_name = key.split('/')[-1]\n",
    "                    \n",
    "                    # Make prediction\n",
    "                    prediction, confidence = predict_gender_from_s3_safe(key, bucket_name, model, processor, detector)\n",
    "                    \n",
    "                    # Store and print results\n",
    "                    result = {\n",
    "                        'image_name': image_name,\n",
    "                        'full_path': key,\n",
    "                        'prediction': prediction,\n",
    "                        'confidence': confidence\n",
    "                    }\n",
    "                    results.append(result)\n",
    "                    \n",
    "                    print(f\"Image: {image_name}\")\n",
    "                    print(f\"Prediction: {prediction}\")\n",
    "                    print(f\"Confidence: {confidence:.3f}\")\n",
    "                    print(\"-\" * 40)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error predicting {key}: {e}\")\n",
    "                    print(\"-\" * 40)\n",
    "    \n",
    "    print(f\"\\nTotal images processed: {len(results)}\")\n",
    "    return results\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "# Load the detector to detect mutlti people pictures\n",
    "# detector = pipeline(\"object-detection\", model=\"facebook/detr-resnet-50\")\n",
    "# detector = pipeline(\"object-detection\", model=\"facebook/detr-resnet-50\", \n",
    "#                         aggregation_strategy=\"simple\")\n",
    "detector = pipeline(\"object-detection\", model=\"facebook/detr-resnet-50\")\n",
    "\n",
    "results = predict_all_images_in_folder('test/', BUCKET_NAME, model, processor, detector)\n",
    "\n",
    "# Optional: Print summary\n",
    "print(\"\\nSUMMARY:\")\n",
    "male_count = sum(1 for r in results if r['prediction'] == 'male')\n",
    "female_count = sum(1 for r in results if r['prediction'] == 'female')\n",
    "avg_confidence = sum(r['confidence'] for r in results) / len(results) if results else 0\n",
    "\n",
    "print(f\"Male predictions: {male_count}\")\n",
    "print(f\"Female predictions: {female_count}\")\n",
    "print(f\"Average confidence: {avg_confidence:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f604cb-b7a9-4bf6-be4c-009fba046ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
